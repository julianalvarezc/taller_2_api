{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c626a4b",
   "metadata": {},
   "source": [
    "Modelo de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "859ca939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Africa       0.60      0.40      0.48        15\n",
      "    Americas       0.36      0.67      0.47         6\n",
      "        Asia       0.67      0.62      0.64        13\n",
      "      Europe       0.46      0.75      0.57         8\n",
      "     Oceania       0.67      0.29      0.40         7\n",
      "\n",
      "    accuracy                           0.53        49\n",
      "   macro avg       0.55      0.54      0.51        49\n",
      "weighted avg       0.58      0.53      0.52        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src') \n",
    "from app import get_api_data\n",
    "\n",
    "api = get_api_data()\n",
    "\n",
    "# 2. Extraer campos relevantes\n",
    "rows = []\n",
    "for country in api:\n",
    "    name = country.get('name', {}).get('common', 'N/A')\n",
    "    capital = country.get('capital', ['N/A'])[0]\n",
    "    region = country.get('region', 'N/A')\n",
    "    population = country.get('population', None)\n",
    "    area = country.get('area', None)\n",
    "    languages = ', '.join(country.get('languages', {}).values()) if 'languages' in country else None\n",
    "\n",
    "    # Validamos que existan todos los campos numéricos y categóricos necesarios\n",
    "    if region and population and area and languages:\n",
    "        rows.append({\n",
    "            'País': name,\n",
    "            'Capital': capital,\n",
    "            'Región': region,\n",
    "            'Población': population,\n",
    "            'Área (km²)': area,\n",
    "            'Idiomas': languages\n",
    "        })\n",
    "\n",
    "# 3. Crear DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# 4. Feature engineering\n",
    "df['Densidad'] = df['Población'] / df['Área (km²)']  # nueva columna: densidad poblacional\n",
    "df['Idioma_principal'] = df['Idiomas'].apply(lambda x: x.split(',')[0].strip())  # primer idioma\n",
    "\n",
    "# 5. Filtrar regiones válidas\n",
    "df = df[df['Región'].isin(['Africa', 'Asia', 'Europe', 'Oceania', 'Americas'])]\n",
    "\n",
    "# 6. One-hot encoding del idioma principal\n",
    "idiomas_dummies = pd.get_dummies(df['Idioma_principal'], prefix='lang')\n",
    "\n",
    "# 7. Variables numéricas + codificadas\n",
    "X = pd.concat([df[['Población', 'Área (km²)', 'Densidad']], idiomas_dummies], axis=1)\n",
    "y = df['Región']\n",
    "\n",
    "# 8. Escalado de variables numéricas\n",
    "scaler = StandardScaler()\n",
    "X[['Población', 'Área (km²)', 'Densidad']] = scaler.fit_transform(X[['Población', 'Área (km²)', 'Densidad']])\n",
    "\n",
    "# 9. División en entrenamiento/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 10. Entrenar modelo\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 11. Predicción y evaluación\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba2a78",
   "metadata": {},
   "source": [
    "Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1086c854",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\Taller_2_api\\Lib\\site-packages\\requests\\models.py:974\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m api = \u001b[43mget_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 2. Extraer campos relevantes\u001b[39;00m\n\u001b[32m      6\u001b[39m rows = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\notebooks\\../src\\app.py:9\u001b[39m, in \u001b[36mget_api_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m url = \u001b[33m\"\u001b[39m\u001b[33mhttps://restcountries.com/v3.1/all\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m response = requests.get(url)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\Taller_2_api\\Lib\\site-packages\\requests\\models.py:978\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "api = get_api_data()\n",
    "\n",
    "\n",
    "# 2. Extraer campos relevantes\n",
    "rows = []\n",
    "for country in api:\n",
    "    name_common = country.get('name', {}).get('common', 'N/A')\n",
    "    independent = country.get('independent', 'N/A')\n",
    "    un_member = country.get('unMember', 'N/A')\n",
    "    capital = country.get('capital', ['N/A'])[0]\n",
    "    region = country.get('region', 'N/A')\n",
    "    subregion = country.get('subregion', 'N/A')\n",
    "    languages = ', '.join(country.get('languages', {}).values()) if 'languages' in country else 'N/A'\n",
    "    latlng = country.get('latlng', 'N/A')\n",
    "    landlocked = country.get('landlocked', 'N/A')\n",
    "    area = country.get('area', 'N/A')\n",
    "    population = country.get('population', 'N/A')\n",
    "    gini_2017 = country.get('gini', {}).get('2017', 'N/A')\n",
    "    car_side = country.get('car', {}).get('side', 'N/A') if 'car' in country else 'N/A'\n",
    "    timezones = ', '.join(country.get('timezones', []))\n",
    "    continents = ', '.join(country.get('continents', []))\n",
    "    start_of_week = country.get('startOfWeek', 'N/A')\n",
    "    \n",
    "    rows.append({\n",
    "        'name.common': name_common,\n",
    "        'Independent': independent,\n",
    "        'unMember': un_member,\n",
    "        'capital': capital,\n",
    "        'region': region,\n",
    "        'subregion': subregion,\n",
    "        'languaje': languages,\n",
    "        'latlng': latlng,\n",
    "        'landlocked': landlocked,\n",
    "        'area': area,\n",
    "        'population': population,\n",
    "        'gini.2017': gini_2017,\n",
    "        'car.side': car_side,\n",
    "        'timezones': timezones,\n",
    "        'continents': continents,\n",
    "        'startOfWeek': start_of_week\n",
    "    })\n",
    "\n",
    "# 3. Crear el DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# 4. Mostrar el DataFrame de manera más ordenada y visual\n",
    "pd.set_option('display.max_columns', None)  # Asegura que todas las columnas se muestren\n",
    "pd.set_option('display.width', 1000)        # Aumenta el ancho de la visualización\n",
    "pd.set_option('display.max_rows', 253)      # Limita la cantidad de filas mostradas\n",
    "\n",
    "# 5. Mostrar las primeras filas del DataFrame\n",
    "print(df.head(253))  # Puedes ajustar el número de filas que quieres ver\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f5ab0",
   "metadata": {},
   "source": [
    "Crear CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ac4eee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo CSV generado: paises_con_borders.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "api = get_api_data()\n",
    "\n",
    "# Extraer los datos deseados\n",
    "records = []\n",
    "for country in api:\n",
    "    record = {\n",
    "        \"name.common\": country.get(\"name\", {}).get(\"common\", None),\n",
    "        \"independent\": country.get(\"independent\", None),\n",
    "        \"unMember\": country.get(\"unMember\", None),\n",
    "        \"capital\": \", \".join(country.get(\"capital\", [])) if country.get(\"capital\") else None,\n",
    "        \"region\": country.get(\"region\", None),\n",
    "        \"subregion\": country.get(\"subregion\", None),\n",
    "        \"language\": \", \".join(country.get(\"languages\", {}).values()) if country.get(\"languages\") else None,\n",
    "        \"latlng\": str(country.get(\"latlng\", None)),\n",
    "        \"landlocked\": country.get(\"landlocked\", None),\n",
    "        \"area\": country.get(\"area\", None),\n",
    "        \"population\": country.get(\"population\", None),\n",
    "        \"borders\": \", \".join(country.get(\"borders\", [])) if country.get(\"borders\") else None,\n",
    "        \"car.side\": country.get(\"car\", {}).get(\"side\", None),\n",
    "        \"timezones\": \", \".join(country.get(\"timezones\", [])) if country.get(\"timezones\") else None,\n",
    "        \"continents\": \", \".join(country.get(\"continents\", [])) if country.get(\"continents\") else None,\n",
    "        \"startOfWeek\": country.get(\"startOfWeek\", None)\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "# Guardar en archivo CSV\n",
    "with open(\"paises_con_borders.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=records[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(records)\n",
    "\n",
    "print(\"✅ Archivo CSV generado: paises_con_borders.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d3a3d",
   "metadata": {},
   "source": [
    "Modelo (aun no terminado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe712aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        40\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "['name.common', 'independent', 'unMember', 'capital', 'region', 'subregion', 'language', 'latlng', 'landlocked', 'area', 'population', 'gini.2017', 'car.side', 'timezones', 'continents', 'startOfWeek']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Cargar el dataset\n",
    "df = pd.read_csv('paises_todos_atributos.csv')\n",
    "\n",
    "# 2. Eliminar columnas no útiles o duplicadas si existen\n",
    "df = df.drop(columns=[col for col in df.columns if 'unnamed' in col.lower()], errors='ignore')\n",
    "\n",
    "# 3. Codificar variables categóricas\n",
    "label_cols = ['name.common', 'independent', 'unMember', 'capital', 'region', 'subregion', 'language', 'latlng', 'landlocked', 'area', 'population', 'gini.2017', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "le_dict = {}\n",
    "\n",
    "for col in label_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = df[col].fillna('Desconocido').astype(str)\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    le_dict[col] = le  # Guardamos los encoders por si queremos predecir luego\n",
    "\n",
    "# 4. Rellenar valores nulos restantes\n",
    "df['gini.2017'] = df['gini.2017'].fillna(df['gini.2017'].median())\n",
    "\n",
    "# 5. Definir variables predictoras (X) y variable objetivo (y)\n",
    "X = df.drop(columns=['unMember'])  # Variable objetivo\n",
    "y = df['unMember']\n",
    "\n",
    "# 6. Separar en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7. Entrenar el modelo\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 8. Evaluar\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(df.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d04cce",
   "metadata": {},
   "source": [
    "ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e859aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name.common  independent  unMember     capital  region  subregion  \\\n",
      "0          Botswana         True      True    Gaborone       0         19   \n",
      "1             Tonga         True      True  Nuku'alofa       5         15   \n",
      "2            Greece         True      True      Athens       4         21   \n",
      "3  Marshall Islands         True      True      Majuro       5         10   \n",
      "4           Belarus         True      True       Minsk       4          8   \n",
      "\n",
      "   language  latlng  landlocked      area  population  \\\n",
      "0        75      32        True  582000.0     2351625   \n",
      "1        74      24       False     747.0      105697   \n",
      "2        93     164       False  131990.0    10715549   \n",
      "3        62     248       False     181.0       59194   \n",
      "4        18     216        True  207600.0     9398861   \n",
      "\n",
      "                   borders  car.side  timezones  continents  startOfWeek  \n",
      "0       NAM, ZAF, ZMB, ZWE         0          4           0            0  \n",
      "1                      NaN         0         28           6            0  \n",
      "2       ALB, BGR, TUR, MKD         1          4           3            0  \n",
      "3                      NaN         1         26           6            0  \n",
      "4  LVA, LTU, POL, RUS, UKR         1          5           3            0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Cargar tu conjunto de datos\n",
    "df = pd.read_csv('paises_con_borders.csv')\n",
    "\n",
    "# 2. Crear un diccionario de LabelEncoders para cada columna categórica\n",
    "columns_to_encode = ['region', 'subregion', 'language', 'car.side', 'timezones', 'continents', 'startOfWeek', 'latlng']  # Ajustamos para incluir 'latlng'\n",
    "le_dict = {}\n",
    "\n",
    "# 3. Ajustar LabelEncoder para cada columna categórica\n",
    "for col in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = df[col].fillna('Desconocido')  # Reemplazar valores NaN por 'Desconocido'\n",
    "    \n",
    "    # Si la columna es 'latlng' y tiene coordenadas (como una lista de latitud y longitud)\n",
    "    if col == 'latlng':\n",
    "        # Para latlng, si es una cadena de coordenadas separadas por coma (ejemplo: '12.34,-56.78'), podemos convertirlo en una cadena única\n",
    "        df[col] = df[col].apply(lambda x: x if isinstance(x, str) else 'Desconocido')  # Si no es una cadena, lo marcamos como 'Desconocido'\n",
    "    \n",
    "    # Ajustar y transformar los datos\n",
    "    df[col] = le.fit_transform(df[col])  # Ajustar y transformar los datos\n",
    "    le_dict[col] = le  # Guardar el encoder para futuras predicciones\n",
    "\n",
    "# 4. Guardar los encoders para usarlos en el futuro\n",
    "joblib.dump(le_dict, 'encoders.pickle')\n",
    "\n",
    "# 5. Verifica que el DataFrame ahora tiene las columnas transformadas\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2c3047",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['gini.2017'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m➡ El modelo predice que este país \u001b[39m\u001b[33m\"\u001b[39m +\n\u001b[32m     51\u001b[39m           (\u001b[33m\"\u001b[39m\u001b[33m🟢 DEBERÍA\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m🔴 NO DEBERÍA\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33m ser miembro de las Naciones Unidas.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# 6. Ejecutar la función\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mpredecir_pais\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mpredecir_pais\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Solo tomar las columnas necesarias\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m input_df = \u001b[43minput_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Convertir tipos según necesidad\u001b[39;00m\n\u001b[32m     34\u001b[39m input_df[\u001b[33m'\u001b[39m\u001b[33marea\u001b[39m\u001b[33m'\u001b[39m] = input_df[\u001b[33m'\u001b[39m\u001b[33marea\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\Taller_2_api\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\Taller_2_api\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\Taller_2_api\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['gini.2017'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# 1. Cargar el modelo entrenado y los encoders\n",
    "model = joblib.load(\"modelo_un.pickle\")\n",
    "le_dict = joblib.load(\"encoders.pickle\")\n",
    "\n",
    "# 2. Cargar el CSV con los datos de los países\n",
    "df = pd.read_csv('paises_con_borders.csv')  # Reemplaza con el nombre de tu archivo CSV\n",
    "\n",
    "# 3. Columnas requeridas\n",
    "cols = ['name.common', 'independent', 'capital', 'region', 'subregion', 'language', 'latlng', 'landlocked',\n",
    "        'area', 'population', 'gini.2017', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "\n",
    "# 4. Especifica aquí el nombre del país para hacer la predicción\n",
    "pais = \"Chile\"  # Cambia \"Chile\" por el nombre del país que deseas predecir\n",
    "\n",
    "# 5. Función para predecir si un país debería ser miembro de la ONU\n",
    "def predecir_pais():\n",
    "    # Filtrar el dataframe para obtener los datos del país\n",
    "    input_df = df[df['name.common'] == pais]\n",
    "\n",
    "    if input_df.empty:\n",
    "        print(f\"\\n🔴 No se encontró el país '{pais}' en los datos.\")\n",
    "        return\n",
    "\n",
    "    # Solo tomar las columnas necesarias\n",
    "    input_df = input_df[cols]\n",
    "\n",
    "    # Convertir tipos según necesidad\n",
    "    input_df['area'] = input_df['area'].astype(float)\n",
    "    input_df['population'] = input_df['population'].astype(int)\n",
    "    input_df['gini.2017'] = input_df['gini.2017'].fillna(0.0).astype(float)\n",
    "\n",
    "    # Aplicar los mismos encoders\n",
    "    for col in le_dict:\n",
    "        if col in input_df.columns:\n",
    "            le = le_dict[col]\n",
    "            input_df[col] = input_df[col].fillna('Desconocido').astype(str)\n",
    "            input_df[col] = le.transform(input_df[col])\n",
    "\n",
    "    # Asegurar que tenga las columnas correctas\n",
    "    input_df = input_df[model.feature_names_in_]\n",
    "\n",
    "    # Predecir\n",
    "    pred = model.predict(input_df)[0]\n",
    "    print(\"\\n➡ El modelo predice que este país \" +\n",
    "          (\"🟢 DEBERÍA\" if pred == 1 else \"🔴 NO DEBERÍA\") + \" ser miembro de las Naciones Unidas.\")\n",
    "\n",
    "# 6. Ejecutar la función\n",
    "predecir_pais()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f762e653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔴 Algunas columnas necesarias no están presentes en el conjunto de datos de prueba.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Cargar el modelo entrenado y los encoders (si los tienes guardados)\n",
    "# Si aún no tienes el archivo de encoders, no lo cargues, creémoslo al vuelo\n",
    "try:\n",
    "    model = joblib.load(\"modelo_un.pickle\")\n",
    "    le_dict = joblib.load(\"encoders.pickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"🔴 No se encontraron los archivos del modelo o los encoders guardados.\")\n",
    "    le_dict = {}\n",
    "\n",
    "# 2. Cargar el CSV con los datos de los países\n",
    "df = pd.read_csv('paises_con_borders.csv')  # Reemplaza con el nombre de tu archivo CSV\n",
    "\n",
    "# 3. Columnas requeridas\n",
    "cols = ['name.common', 'independent','unMember', 'capital', 'region', 'subregion', 'language', 'latlng', 'landlocked',\n",
    "        'area', 'population', 'gini.2017', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "\n",
    "# 4. Especifica aquí el nombre del país para hacer la predicción\n",
    "pais = \"Chile\"  # Cambia \"Chile\" por el nombre del país que deseas predecir\n",
    "\n",
    "# 5. Cargar el conjunto de datos de prueba (si tienes uno)\n",
    "df_test = pd.read_csv('paises_con_borders.csv')  # Reemplaza con tu archivo de prueba\n",
    "\n",
    "# 6. Verificar que las columnas necesarias están presentes en df_test\n",
    "if not all(col in df_test.columns for col in cols):\n",
    "    print(\"🔴 Algunas columnas necesarias no están presentes en el conjunto de datos de prueba.\")\n",
    "else:\n",
    "    # 7. Filtrar solo las columnas necesarias del conjunto de datos de prueba\n",
    "    df_test = df_test[cols]\n",
    "\n",
    "    # Crear LabelEncoders si no existen\n",
    "    columns_to_encode = ['region', 'subregion', 'language', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "    if not le_dict:  # Si no tenemos encoders cargados, los creamos\n",
    "        le_dict = {}\n",
    "        for col in columns_to_encode:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('Desconocido')  # Reemplazar valores NaN por 'Desconocido'\n",
    "            df[col] = le.fit_transform(df[col])  # Ajustar y transformar los datos\n",
    "            le_dict[col] = le  # Guardar el encoder para futuras predicciones\n",
    "        # Guardar los encoders\n",
    "        joblib.dump(le_dict, 'encoders.pickle')\n",
    "\n",
    "    # 8. Función para predecir si un país debería ser miembro de la ONU\n",
    "    def predecir_pais():\n",
    "        # Filtrar el dataframe para obtener los datos del país\n",
    "        input_df = df[df['name.common'] == pais]\n",
    "\n",
    "        if input_df.empty:\n",
    "            print(f\"\\n🔴 No se encontró el país '{pais}' en los datos.\")\n",
    "            return\n",
    "\n",
    "        # Solo tomar las columnas necesarias\n",
    "        input_df = input_df[cols]\n",
    "\n",
    "        # Convertir tipos según necesidad\n",
    "        input_df['area'] = input_df['area'].astype(float)\n",
    "        input_df['population'] = input_df['population'].astype(int)\n",
    "        input_df['gini.2017'] = input_df['gini.2017'].fillna(0.0).astype(float)\n",
    "\n",
    "        # Aplicar los mismos encoders\n",
    "        for col in le_dict:\n",
    "            if col in input_df.columns:\n",
    "                le = le_dict[col]\n",
    "                input_df[col] = input_df[col].fillna('Desconocido').astype(str)\n",
    "                input_df[col] = le.transform(input_df[col])\n",
    "\n",
    "        # Asegurar que tenga las columnas correctas\n",
    "        input_df = input_df[model.feature_names_in_]\n",
    "\n",
    "        # Predecir\n",
    "        pred = model.predict(input_df)[0]\n",
    "        print(\"\\n➡ El modelo predice que este país \" +\n",
    "              (\"🟢 DEBERÍA\" if pred == 1 else \"🔴 NO DEBERÍA\") + \" ser miembro de las Naciones Unidas.\")\n",
    "\n",
    "        # Asegurarse de que las columnas categóricas de df_test estén correctamente transformadas\n",
    "        for col in le_dict:\n",
    "            if col in df_test.columns:\n",
    "                le = le_dict[col]\n",
    "                df_test[col] = df_test[col].fillna('Desconocido').astype(str)\n",
    "                df_test[col] = le.transform(df_test[col])\n",
    "\n",
    "        # Convertir las columnas numéricas de texto a números\n",
    "        df_test['area'] = df_test['area'].astype(float)\n",
    "        df_test['population'] = df_test['population'].astype(int)\n",
    "        df_test['gini.2017'] = df_test['gini.2017'].fillna(0.0).astype(float)\n",
    "\n",
    "        # Asegurarse de que las columnas estén alineadas con las características de entrada del modelo\n",
    "        df_test = df_test[model.feature_names_in_]\n",
    "\n",
    "        # Verificar que la columna 'unMember' esté en el conjunto de prueba\n",
    "        if 'unMember' not in df_test.columns:\n",
    "            print(\"🔴 La columna 'unMember' no está presente en el conjunto de prueba.\")\n",
    "            return\n",
    "\n",
    "        # Calcular y mostrar las métricas de clasificación en el conjunto de datos de prueba\n",
    "        y_true = df_test['unMember']  # Asegúrate de que la columna 'unMember' esté en tus datos de prueba\n",
    "        y_pred = model.predict(df_test)\n",
    "\n",
    "        print(\"\\nMétricas de rendimiento:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # 9. Ejecutar la función\n",
    "    predecir_pais()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b350e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "➡ El modelo predice que este país 🔴 NO DEBERÍA ser miembro de las Naciones Unidas.\n",
      "\n",
      "📊 Métricas del modelo:\n",
      "Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00        58\n",
      "        True       1.00      1.00      1.00       192\n",
      "\n",
      "    accuracy                           1.00       250\n",
      "   macro avg       1.00      1.00      1.00       250\n",
      "weighted avg       1.00      1.00      1.00       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Cargar modelo y encoders\n",
    "def cargar_modelo_y_encoders():\n",
    "    try:\n",
    "        model = joblib.load(\"modelo_un.pickle\")\n",
    "        le_dict = joblib.load(\"encoders.pickle\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"🔴 No se encontraron los archivos del modelo o los encoders guardados.\")\n",
    "        model = None\n",
    "        le_dict = {}\n",
    "    return model, le_dict\n",
    "\n",
    "# 2. Preparar el dataset\n",
    "def preparar_datos(filepath, cols, le_dict):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    if not all(col in df.columns for col in cols):\n",
    "        print(\"🔴 Algunas columnas necesarias no están presentes en el conjunto de datos.\")\n",
    "        return None, None\n",
    "\n",
    "    df = df[cols].copy()\n",
    "    \n",
    "    # Codificar variables categóricas\n",
    "    columns_to_encode = ['region', 'subregion', 'language', 'car.side', 'timezones', 'continents', 'startOfWeek', 'latlng']\n",
    "    if not le_dict:\n",
    "        le_dict = {}\n",
    "        for col in columns_to_encode:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('Desconocido').astype(str)\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            le_dict[col] = le\n",
    "        joblib.dump(le_dict, 'encoders.pickle')\n",
    "    else:\n",
    "        for col in le_dict:\n",
    "            if col in df.columns:\n",
    "                le = le_dict[col]\n",
    "                df[col] = df[col].fillna('Desconocido').astype(str)\n",
    "                df[col] = le.transform(df[col])\n",
    "\n",
    "    # Asegurar tipo numérico\n",
    "    df['area'] = df['area'].astype(float)\n",
    "    df['population'] = df['population'].astype(int)\n",
    "\n",
    "    return df, le_dict\n",
    "\n",
    "# 3. Función de predicción\n",
    "def predecir_pais(pais, model, df_full, le_dict):\n",
    "    input_df = df_full[df_full['name.common'] == pais].copy()\n",
    "\n",
    "    if input_df.empty:\n",
    "        print(f\"\\n🔴 No se encontró el país '{pais}' en los datos.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        input_df = input_df[model.feature_names_in_]\n",
    "        pred = model.predict(input_df)[0]\n",
    "        print(\"\\n➡ El modelo predice que este país \" +\n",
    "              (\"🟢 DEBERÍA\" if pred == 1 else \"🔴 NO DEBERÍA\") +\n",
    "              \" ser miembro de las Naciones Unidas.\")\n",
    "    except Exception as e:\n",
    "        print(\"🔴 Error durante la predicción:\", e)\n",
    "\n",
    "# 4. Evaluación del modelo completo\n",
    "def evaluar_modelo(model, df_test):\n",
    "    if 'unMember' not in df_test.columns:\n",
    "        print(\"🔴 La columna 'unMember' no está presente en el conjunto de prueba.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        y_true = df_test['unMember'].copy()  # ← Guardar antes de eliminar columnas\n",
    "        df_test = df_test[model.feature_names_in_]\n",
    "        y_pred = model.predict(df_test)\n",
    "\n",
    "        print(\"\\n📊 Métricas del modelo:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    except Exception as e:\n",
    "        print(\"🔴 Error al evaluar el modelo:\", e)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Ejecutar todo el pipeline\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    cols = ['name.common', 'independent', 'unMember', 'capital', 'region', 'subregion', 'language', 'latlng',\n",
    "            'landlocked', 'area', 'population', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "    pais = \"Hong Kong\"  # Cambia el país aquí si lo deseas\n",
    "    filepath = 'paises_con_borders.csv'\n",
    "\n",
    "    model, le_dict = cargar_modelo_y_encoders()\n",
    "\n",
    "    if model:\n",
    "        df_full, le_dict = preparar_datos(filepath, cols, le_dict)\n",
    "        if df_full is not None:\n",
    "            predecir_pais(pais, model, df_full, le_dict)\n",
    "            evaluar_modelo(model, df_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b6140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name.common  independent  unMember     capital  region  subregion  \\\n",
      "0          Botswana         True      True    Gaborone       0         19   \n",
      "1             Tonga         True      True  Nuku'alofa       5         15   \n",
      "2            Greece         True      True      Athens       4         21   \n",
      "3  Marshall Islands         True      True      Majuro       5         10   \n",
      "4           Belarus         True      True       Minsk       4          8   \n",
      "\n",
      "   language  latlng  landlocked      area  population  \\\n",
      "0        75      32        True  582000.0     2351625   \n",
      "1        74      24       False     747.0      105697   \n",
      "2        93     164       False  131990.0    10715549   \n",
      "3        62     248       False     181.0       59194   \n",
      "4        18     216        True  207600.0     9398861   \n",
      "\n",
      "                   borders  car.side  timezones  continents  startOfWeek  \n",
      "0       NAM, ZAF, ZMB, ZWE         0          4           0            0  \n",
      "1                      NaN         0         28           6            0  \n",
      "2       ALB, BGR, TUR, MKD         1          4           3            0  \n",
      "3                      NaN         1         26           6            0  \n",
      "4  LVA, LTU, POL, RUS, UKR         1          5           3            0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Cargar tu conjunto de datos\n",
    "df = pd.read_csv('paises_con_borders.csv')\n",
    "\n",
    "# 2. Crear un diccionario de LabelEncoders para cada columna categórica\n",
    "columns_to_encode = ['region', 'subregion', 'language', 'car.side', 'timezones', 'continents', 'startOfWeek', 'latlng']\n",
    "le_dict = {}\n",
    "\n",
    "# 3. Ajustar LabelEncoder para cada columna categórica\n",
    "for col in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = df[col].fillna('Desconocido').astype(str)  # Reemplazar NaN y asegurar tipo string\n",
    "    df[col] = le.fit_transform(df[col])  # Ajustar y transformar los datos\n",
    "    le_dict[col] = le  # Guardar el encoder para futuras predicciones\n",
    "\n",
    "# 4. Guardar los encoders para usarlos en el futuro\n",
    "joblib.dump(le_dict, 'encoders.pickle')\n",
    "\n",
    "# 5. Verifica que el DataFrame ahora tiene las columnas transformadas\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8464e51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "➡ El modelo predice que este país 🔴 NO DEBERÍA ser miembro de las Naciones Unidas.\n",
      "\n",
      "📊 Métricas del modelo:\n",
      "Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00        58\n",
      "        True       1.00      1.00      1.00       192\n",
      "\n",
      "    accuracy                           1.00       250\n",
      "   macro avg       1.00      1.00      1.00       250\n",
      "weighted avg       1.00      1.00      1.00       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Cargar modelo y encoders\n",
    "def cargar_modelo_y_encoders():\n",
    "    try:\n",
    "        model = joblib.load(\"modelo_un.pickle\")\n",
    "        le_dict = joblib.load(\"encoders.pickle\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"🔴 No se encontraron los archivos del modelo o los encoders guardados.\")\n",
    "        model = None\n",
    "        le_dict = {}\n",
    "    return model, le_dict\n",
    "\n",
    "# 2. Preparar el dataset\n",
    "def preparar_datos(filepath, cols, le_dict):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    if not all(col in df.columns for col in cols):\n",
    "        print(\"🔴 Algunas columnas necesarias no están presentes en el conjunto de datos.\")\n",
    "        return None, None, None\n",
    "\n",
    "    df = df[cols].copy()\n",
    "\n",
    "    # Guardar target y quitarlo de las features\n",
    "    y_true = df['unMember'].copy()\n",
    "    df = df.drop(columns=['unMember'])\n",
    "\n",
    "    # Codificar variables categóricas\n",
    "    columns_to_encode = ['region', 'subregion', 'language', 'car.side', 'timezones', 'continents', 'startOfWeek', 'latlng']\n",
    "    if not le_dict:\n",
    "        le_dict = {}\n",
    "        for col in columns_to_encode:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('Desconocido').astype(str)\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            le_dict[col] = le\n",
    "        joblib.dump(le_dict, 'encoders.pickle')\n",
    "    else:\n",
    "        for col in le_dict:\n",
    "            if col in df.columns:\n",
    "                le = le_dict[col]\n",
    "                df[col] = df[col].fillna('Desconocido').astype(str)\n",
    "                df[col] = le.transform(df[col])\n",
    "\n",
    "    # Asegurar tipo numérico\n",
    "    df['area'] = df['area'].astype(float)\n",
    "    df['population'] = df['population'].astype(int)\n",
    "\n",
    "    return df, y_true, le_dict\n",
    "\n",
    "# 3. Función de predicción\n",
    "def predecir_pais(pais, model, df_full_original, le_dict):\n",
    "    input_df = df_full_original[df_full_original['name.common'] == pais].copy()\n",
    "\n",
    "    if input_df.empty:\n",
    "        print(f\"\\n🔴 No se encontró el país '{pais}' en los datos.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        input_df = input_df[model.feature_names_in_]\n",
    "        pred = model.predict(input_df)[0]\n",
    "        print(\"\\n➡ El modelo predice que este país \" +\n",
    "              (\"🟢 DEBERÍA\" if pred == 1 else \"🔴 NO DEBERÍA\") +\n",
    "              \" ser miembro de las Naciones Unidas.\")\n",
    "    except Exception as e:\n",
    "        print(\"🔴 Error durante la predicción:\", e)\n",
    "\n",
    "# 4. Evaluación del modelo completo\n",
    "def evaluar_modelo(model, df_test, y_true):\n",
    "    try:\n",
    "        df_test = df_test[model.feature_names_in_]\n",
    "        y_pred = model.predict(df_test)\n",
    "\n",
    "        print(\"\\n📊 Métricas del modelo:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    except Exception as e:\n",
    "        print(\"🔴 Error al evaluar el modelo:\", e)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Ejecutar todo el pipeline\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    cols = ['name.common', 'independent', 'unMember', 'capital', 'region', 'subregion', 'language', 'latlng',\n",
    "            'landlocked', 'area', 'population', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "    pais = \"Sint Maarten\"  # Cambia el país aquí si lo deseas\n",
    "    filepath = 'paises_con_borders.csv'\n",
    "\n",
    "    model, le_dict = cargar_modelo_y_encoders()\n",
    "\n",
    "    if model:\n",
    "        df_features, y_true, le_dict = preparar_datos(filepath, cols, le_dict)\n",
    "        if df_features is not None:\n",
    "            predecir_pais(pais, model, df_features, le_dict)\n",
    "            evaluar_modelo(model, df_features, y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69122ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo CSV generado: paises_con_lat_lon.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extraer los datos deseados\n",
    "records = []\n",
    "for country in api:\n",
    "    latlng = country.get(\"latlng\", [None, None])\n",
    "    lat = latlng[0] if len(latlng) > 0 else None\n",
    "    lng = latlng[1] if len(latlng) > 1 else None\n",
    "\n",
    "    record = {\n",
    "        \"name.common\": country.get(\"name\", {}).get(\"common\", None),\n",
    "        \"independent\": country.get(\"independent\", None),\n",
    "        \"unMember\": country.get(\"unMember\", None),\n",
    "        \"capital\": \", \".join(country.get(\"capital\", [])) if country.get(\"capital\") else None,\n",
    "        \"region\": country.get(\"region\", None),\n",
    "        \"subregion\": country.get(\"subregion\", None),\n",
    "        \"language\": \", \".join(country.get(\"languages\", {}).values()) if country.get(\"languages\") else None,\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lng,\n",
    "        \"landlocked\": country.get(\"landlocked\", None),\n",
    "        \"area\": country.get(\"area\", None),\n",
    "        \"population\": country.get(\"population\", None),\n",
    "        \"borders\": \", \".join(country.get(\"borders\", [])) if country.get(\"borders\") else None,\n",
    "        \"car.side\": country.get(\"car\", {}).get(\"side\", None),\n",
    "        \"timezones\": \", \".join(country.get(\"timezones\", [])) if country.get(\"timezones\") else None,\n",
    "        \"continents\": \", \".join(country.get(\"continents\", [])) if country.get(\"continents\") else None,\n",
    "        \"startOfWeek\": country.get(\"startOfWeek\", None)\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "# Guardar en archivo CSV\n",
    "with open(\"paises_con_lat_lon.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=records[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(records)\n",
    "\n",
    "print(\"✅ Archivo CSV generado: paises_con_lat_lon.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "837db011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo CSV generado: paises_numericos.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Diccionarios para codificar variables categóricas\n",
    "region_codes = {}\n",
    "subregion_codes = {}\n",
    "language_codes = {}\n",
    "capital_codes = {}\n",
    "continent_codes = {}\n",
    "start_of_week_codes = {}\n",
    "car_side_codes = {}\n",
    "\n",
    "def get_code(value, code_dict):\n",
    "    if value not in code_dict:\n",
    "        code_dict[value] = len(code_dict) + 1\n",
    "    return code_dict[value]\n",
    "\n",
    "# Extraer y codificar datos\n",
    "records = []\n",
    "for country in api:\n",
    "    latlng = country.get(\"latlng\", [None, None])\n",
    "    lat = latlng[0] if len(latlng) > 0 else None\n",
    "    lng = latlng[1] if len(latlng) > 1 else None\n",
    "\n",
    "    record = {\n",
    "        \"independent\": int(country.get(\"independent\", False)),\n",
    "        \"unMember\": int(country.get(\"unMember\", False)),\n",
    "        \"capital_code\": get_code(\", \".join(country.get(\"capital\", [])) if country.get(\"capital\") else \"None\", capital_codes),\n",
    "        \"region_code\": get_code(country.get(\"region\", \"None\"), region_codes),\n",
    "        \"subregion_code\": get_code(country.get(\"subregion\", \"None\"), subregion_codes),\n",
    "        \"language_code\": get_code(\", \".join(country.get(\"languages\", {}).values()) if country.get(\"languages\") else \"None\", language_codes),\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lng,\n",
    "        \"landlocked\": int(country.get(\"landlocked\", False)),\n",
    "        \"area\": country.get(\"area\", 0),\n",
    "        \"population\": country.get(\"population\", 0),\n",
    "        \"borders_count\": len(country.get(\"borders\", [])),\n",
    "        \"car_side_code\": get_code(country.get(\"car\", {}).get(\"side\", \"None\"), car_side_codes),\n",
    "        \"timezones_count\": len(country.get(\"timezones\", [])),\n",
    "        \"continents_code\": get_code(\", \".join(country.get(\"continents\", [])) if country.get(\"continents\") else \"None\", continent_codes),\n",
    "        \"startOfWeek_code\": get_code(country.get(\"startOfWeek\", \"None\"), start_of_week_codes)\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "# Guardar en archivo CSV\n",
    "with open(\"paises_numericos.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=records[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(records)\n",
    "\n",
    "print(\"✅ Archivo CSV generado: paises_numericos.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Taller_2_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
