{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c626a4b",
   "metadata": {},
   "source": [
    "Modelo de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "859ca939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Africa       0.60      0.40      0.48        15\n",
      "    Americas       0.36      0.67      0.47         6\n",
      "        Asia       0.67      0.62      0.64        13\n",
      "      Europe       0.46      0.75      0.57         8\n",
      "     Oceania       0.67      0.29      0.40         7\n",
      "\n",
      "    accuracy                           0.53        49\n",
      "   macro avg       0.55      0.54      0.51        49\n",
      "weighted avg       0.58      0.53      0.52        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src') \n",
    "from app import get_api_data\n",
    "\n",
    "api = get_api_data()\n",
    "\n",
    "# 2. Extraer campos relevantes\n",
    "rows = []\n",
    "for country in api:\n",
    "    name = country.get('name', {}).get('common', 'N/A')\n",
    "    capital = country.get('capital', ['N/A'])[0]\n",
    "    region = country.get('region', 'N/A')\n",
    "    population = country.get('population', None)\n",
    "    area = country.get('area', None)\n",
    "    languages = ', '.join(country.get('languages', {}).values()) if 'languages' in country else None\n",
    "\n",
    "    # Validamos que existan todos los campos num√©ricos y categ√≥ricos necesarios\n",
    "    if region and population and area and languages:\n",
    "        rows.append({\n",
    "            'Pa√≠s': name,\n",
    "            'Capital': capital,\n",
    "            'Regi√≥n': region,\n",
    "            'Poblaci√≥n': population,\n",
    "            '√Årea (km¬≤)': area,\n",
    "            'Idiomas': languages\n",
    "        })\n",
    "\n",
    "# 3. Crear DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# 4. Feature engineering\n",
    "df['Densidad'] = df['Poblaci√≥n'] / df['√Årea (km¬≤)']  # nueva columna: densidad poblacional\n",
    "df['Idioma_principal'] = df['Idiomas'].apply(lambda x: x.split(',')[0].strip())  # primer idioma\n",
    "\n",
    "# 5. Filtrar regiones v√°lidas\n",
    "df = df[df['Regi√≥n'].isin(['Africa', 'Asia', 'Europe', 'Oceania', 'Americas'])]\n",
    "\n",
    "# 6. One-hot encoding del idioma principal\n",
    "idiomas_dummies = pd.get_dummies(df['Idioma_principal'], prefix='lang')\n",
    "\n",
    "# 7. Variables num√©ricas + codificadas\n",
    "X = pd.concat([df[['Poblaci√≥n', '√Årea (km¬≤)', 'Densidad']], idiomas_dummies], axis=1)\n",
    "y = df['Regi√≥n']\n",
    "\n",
    "# 8. Escalado de variables num√©ricas\n",
    "scaler = StandardScaler()\n",
    "X[['Poblaci√≥n', '√Årea (km¬≤)', 'Densidad']] = scaler.fit_transform(X[['Poblaci√≥n', '√Årea (km¬≤)', 'Densidad']])\n",
    "\n",
    "# 9. Divisi√≥n en entrenamiento/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 10. Entrenar modelo\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 11. Predicci√≥n y evaluaci√≥n\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba2a78",
   "metadata": {},
   "source": [
    "Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1086c854",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\Taller_2_api\\Lib\\site-packages\\requests\\models.py:974\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m api = \u001b[43mget_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 2. Extraer campos relevantes\u001b[39;00m\n\u001b[32m      6\u001b[39m rows = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\notebooks\\../src\\app.py:9\u001b[39m, in \u001b[36mget_api_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m url = \u001b[33m\"\u001b[39m\u001b[33mhttps://restcountries.com/v3.1/all\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m response = requests.get(url)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\Taller_2_api\\Lib\\site-packages\\requests\\models.py:978\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "api = get_api_data()\n",
    "\n",
    "\n",
    "# 2. Extraer campos relevantes\n",
    "rows = []\n",
    "for country in api:\n",
    "    name_common = country.get('name', {}).get('common', 'N/A')\n",
    "    independent = country.get('independent', 'N/A')\n",
    "    un_member = country.get('unMember', 'N/A')\n",
    "    capital = country.get('capital', ['N/A'])[0]\n",
    "    region = country.get('region', 'N/A')\n",
    "    subregion = country.get('subregion', 'N/A')\n",
    "    languages = ', '.join(country.get('languages', {}).values()) if 'languages' in country else 'N/A'\n",
    "    latlng = country.get('latlng', 'N/A')\n",
    "    landlocked = country.get('landlocked', 'N/A')\n",
    "    area = country.get('area', 'N/A')\n",
    "    population = country.get('population', 'N/A')\n",
    "    gini_2017 = country.get('gini', {}).get('2017', 'N/A')\n",
    "    car_side = country.get('car', {}).get('side', 'N/A') if 'car' in country else 'N/A'\n",
    "    timezones = ', '.join(country.get('timezones', []))\n",
    "    continents = ', '.join(country.get('continents', []))\n",
    "    start_of_week = country.get('startOfWeek', 'N/A')\n",
    "    \n",
    "    rows.append({\n",
    "        'name.common': name_common,\n",
    "        'Independent': independent,\n",
    "        'unMember': un_member,\n",
    "        'capital': capital,\n",
    "        'region': region,\n",
    "        'subregion': subregion,\n",
    "        'languaje': languages,\n",
    "        'latlng': latlng,\n",
    "        'landlocked': landlocked,\n",
    "        'area': area,\n",
    "        'population': population,\n",
    "        'gini.2017': gini_2017,\n",
    "        'car.side': car_side,\n",
    "        'timezones': timezones,\n",
    "        'continents': continents,\n",
    "        'startOfWeek': start_of_week\n",
    "    })\n",
    "\n",
    "# 3. Crear el DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# 4. Mostrar el DataFrame de manera m√°s ordenada y visual\n",
    "pd.set_option('display.max_columns', None)  # Asegura que todas las columnas se muestren\n",
    "pd.set_option('display.width', 1000)        # Aumenta el ancho de la visualizaci√≥n\n",
    "pd.set_option('display.max_rows', 253)      # Limita la cantidad de filas mostradas\n",
    "\n",
    "# 5. Mostrar las primeras filas del DataFrame\n",
    "print(df.head(253))  # Puedes ajustar el n√∫mero de filas que quieres ver\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f5ab0",
   "metadata": {},
   "source": [
    "Crear CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ac4eee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo CSV generado: paises_con_borders.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "api = get_api_data()\n",
    "\n",
    "# Extraer los datos deseados\n",
    "records = []\n",
    "for country in api:\n",
    "    record = {\n",
    "        \"name.common\": country.get(\"name\", {}).get(\"common\", None),\n",
    "        \"independent\": country.get(\"independent\", None),\n",
    "        \"unMember\": country.get(\"unMember\", None),\n",
    "        \"capital\": \", \".join(country.get(\"capital\", [])) if country.get(\"capital\") else None,\n",
    "        \"region\": country.get(\"region\", None),\n",
    "        \"subregion\": country.get(\"subregion\", None),\n",
    "        \"language\": \", \".join(country.get(\"languages\", {}).values()) if country.get(\"languages\") else None,\n",
    "        \"latlng\": str(country.get(\"latlng\", None)),\n",
    "        \"landlocked\": country.get(\"landlocked\", None),\n",
    "        \"area\": country.get(\"area\", None),\n",
    "        \"population\": country.get(\"population\", None),\n",
    "        \"borders\": \", \".join(country.get(\"borders\", [])) if country.get(\"borders\") else None,\n",
    "        \"car.side\": country.get(\"car\", {}).get(\"side\", None),\n",
    "        \"timezones\": \", \".join(country.get(\"timezones\", [])) if country.get(\"timezones\") else None,\n",
    "        \"continents\": \", \".join(country.get(\"continents\", [])) if country.get(\"continents\") else None,\n",
    "        \"startOfWeek\": country.get(\"startOfWeek\", None)\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "# Guardar en archivo CSV\n",
    "with open(\"paises_con_borders.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=records[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(records)\n",
    "\n",
    "print(\"‚úÖ Archivo CSV generado: paises_con_borders.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d3a3d",
   "metadata": {},
   "source": [
    "Modelo (aun no terminado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe712aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        40\n",
      "\n",
      "    accuracy                           1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "['name.common', 'independent', 'unMember', 'capital', 'region', 'subregion', 'language', 'latlng', 'landlocked', 'area', 'population', 'gini.2017', 'car.side', 'timezones', 'continents', 'startOfWeek']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Cargar el dataset\n",
    "df = pd.read_csv('paises_todos_atributos.csv')\n",
    "\n",
    "# 2. Eliminar columnas no √∫tiles o duplicadas si existen\n",
    "df = df.drop(columns=[col for col in df.columns if 'unnamed' in col.lower()], errors='ignore')\n",
    "\n",
    "# 3. Codificar variables categ√≥ricas\n",
    "label_cols = ['name.common', 'independent', 'unMember', 'capital', 'region', 'subregion', 'language', 'latlng', 'landlocked', 'area', 'population', 'gini.2017', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "le_dict = {}\n",
    "\n",
    "for col in label_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = df[col].fillna('Desconocido').astype(str)\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    le_dict[col] = le  # Guardamos los encoders por si queremos predecir luego\n",
    "\n",
    "# 4. Rellenar valores nulos restantes\n",
    "df['gini.2017'] = df['gini.2017'].fillna(df['gini.2017'].median())\n",
    "\n",
    "# 5. Definir variables predictoras (X) y variable objetivo (y)\n",
    "X = df.drop(columns=['unMember'])  # Variable objetivo\n",
    "y = df['unMember']\n",
    "\n",
    "# 6. Separar en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7. Entrenar el modelo\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 8. Evaluar\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(df.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d04cce",
   "metadata": {},
   "source": [
    "ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e859aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name.common  independent  unMember     capital  region  subregion  \\\n",
      "0          Botswana         True      True    Gaborone       0         19   \n",
      "1             Tonga         True      True  Nuku'alofa       5         15   \n",
      "2            Greece         True      True      Athens       4         21   \n",
      "3  Marshall Islands         True      True      Majuro       5         10   \n",
      "4           Belarus         True      True       Minsk       4          8   \n",
      "\n",
      "   language  latlng  landlocked      area  population  \\\n",
      "0        75      32        True  582000.0     2351625   \n",
      "1        74      24       False     747.0      105697   \n",
      "2        93     164       False  131990.0    10715549   \n",
      "3        62     248       False     181.0       59194   \n",
      "4        18     216        True  207600.0     9398861   \n",
      "\n",
      "                   borders  car.side  timezones  continents  startOfWeek  \n",
      "0       NAM, ZAF, ZMB, ZWE         0          4           0            0  \n",
      "1                      NaN         0         28           6            0  \n",
      "2       ALB, BGR, TUR, MKD         1          4           3            0  \n",
      "3                      NaN         1         26           6            0  \n",
      "4  LVA, LTU, POL, RUS, UKR         1          5           3            0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Cargar tu conjunto de datos\n",
    "df = pd.read_csv('paises_con_borders.csv')\n",
    "\n",
    "# 2. Crear un diccionario de LabelEncoders para cada columna categ√≥rica\n",
    "columns_to_encode = ['region', 'subregion', 'language', 'car.side', 'timezones', 'continents', 'startOfWeek', 'latlng']  # Ajustamos para incluir 'latlng'\n",
    "le_dict = {}\n",
    "\n",
    "# 3. Ajustar LabelEncoder para cada columna categ√≥rica\n",
    "for col in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = df[col].fillna('Desconocido')  # Reemplazar valores NaN por 'Desconocido'\n",
    "    \n",
    "    # Si la columna es 'latlng' y tiene coordenadas (como una lista de latitud y longitud)\n",
    "    if col == 'latlng':\n",
    "        # Para latlng, si es una cadena de coordenadas separadas por coma (ejemplo: '12.34,-56.78'), podemos convertirlo en una cadena √∫nica\n",
    "        df[col] = df[col].apply(lambda x: x if isinstance(x, str) else 'Desconocido')  # Si no es una cadena, lo marcamos como 'Desconocido'\n",
    "    \n",
    "    # Ajustar y transformar los datos\n",
    "    df[col] = le.fit_transform(df[col])  # Ajustar y transformar los datos\n",
    "    le_dict[col] = le  # Guardar el encoder para futuras predicciones\n",
    "\n",
    "# 4. Guardar los encoders para usarlos en el futuro\n",
    "joblib.dump(le_dict, 'encoders.pickle')\n",
    "\n",
    "# 5. Verifica que el DataFrame ahora tiene las columnas transformadas\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2c3047",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['gini.2017'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚û° El modelo predice que este pa√≠s \u001b[39m\u001b[33m\"\u001b[39m +\n\u001b[32m     51\u001b[39m           (\u001b[33m\"\u001b[39m\u001b[33müü¢ DEBER√çA\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33müî¥ NO DEBER√çA\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33m ser miembro de las Naciones Unidas.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# 6. Ejecutar la funci√≥n\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mpredecir_pais\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mpredecir_pais\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Solo tomar las columnas necesarias\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m input_df = \u001b[43minput_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Convertir tipos seg√∫n necesidad\u001b[39;00m\n\u001b[32m     34\u001b[39m input_df[\u001b[33m'\u001b[39m\u001b[33marea\u001b[39m\u001b[33m'\u001b[39m] = input_df[\u001b[33m'\u001b[39m\u001b[33marea\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\Taller_2_api\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\Taller_2_api\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jorge\\Desktop\\Repositorio\\taller_2_api\\Taller_2_api\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['gini.2017'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# 1. Cargar el modelo entrenado y los encoders\n",
    "model = joblib.load(\"modelo_un.pickle\")\n",
    "le_dict = joblib.load(\"encoders.pickle\")\n",
    "\n",
    "# 2. Cargar el CSV con los datos de los pa√≠ses\n",
    "df = pd.read_csv('paises_con_borders.csv')  # Reemplaza con el nombre de tu archivo CSV\n",
    "\n",
    "# 3. Columnas requeridas\n",
    "cols = ['name.common', 'independent', 'capital', 'region', 'subregion', 'language', 'latlng', 'landlocked',\n",
    "        'area', 'population', 'gini.2017', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "\n",
    "# 4. Especifica aqu√≠ el nombre del pa√≠s para hacer la predicci√≥n\n",
    "pais = \"Chile\"  # Cambia \"Chile\" por el nombre del pa√≠s que deseas predecir\n",
    "\n",
    "# 5. Funci√≥n para predecir si un pa√≠s deber√≠a ser miembro de la ONU\n",
    "def predecir_pais():\n",
    "    # Filtrar el dataframe para obtener los datos del pa√≠s\n",
    "    input_df = df[df['name.common'] == pais]\n",
    "\n",
    "    if input_df.empty:\n",
    "        print(f\"\\nüî¥ No se encontr√≥ el pa√≠s '{pais}' en los datos.\")\n",
    "        return\n",
    "\n",
    "    # Solo tomar las columnas necesarias\n",
    "    input_df = input_df[cols]\n",
    "\n",
    "    # Convertir tipos seg√∫n necesidad\n",
    "    input_df['area'] = input_df['area'].astype(float)\n",
    "    input_df['population'] = input_df['population'].astype(int)\n",
    "    input_df['gini.2017'] = input_df['gini.2017'].fillna(0.0).astype(float)\n",
    "\n",
    "    # Aplicar los mismos encoders\n",
    "    for col in le_dict:\n",
    "        if col in input_df.columns:\n",
    "            le = le_dict[col]\n",
    "            input_df[col] = input_df[col].fillna('Desconocido').astype(str)\n",
    "            input_df[col] = le.transform(input_df[col])\n",
    "\n",
    "    # Asegurar que tenga las columnas correctas\n",
    "    input_df = input_df[model.feature_names_in_]\n",
    "\n",
    "    # Predecir\n",
    "    pred = model.predict(input_df)[0]\n",
    "    print(\"\\n‚û° El modelo predice que este pa√≠s \" +\n",
    "          (\"üü¢ DEBER√çA\" if pred == 1 else \"üî¥ NO DEBER√çA\") + \" ser miembro de las Naciones Unidas.\")\n",
    "\n",
    "# 6. Ejecutar la funci√≥n\n",
    "predecir_pais()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f762e653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ Algunas columnas necesarias no est√°n presentes en el conjunto de datos de prueba.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Cargar el modelo entrenado y los encoders (si los tienes guardados)\n",
    "# Si a√∫n no tienes el archivo de encoders, no lo cargues, cre√©moslo al vuelo\n",
    "try:\n",
    "    model = joblib.load(\"modelo_un.pickle\")\n",
    "    le_dict = joblib.load(\"encoders.pickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"üî¥ No se encontraron los archivos del modelo o los encoders guardados.\")\n",
    "    le_dict = {}\n",
    "\n",
    "# 2. Cargar el CSV con los datos de los pa√≠ses\n",
    "df = pd.read_csv('paises_con_borders.csv')  # Reemplaza con el nombre de tu archivo CSV\n",
    "\n",
    "# 3. Columnas requeridas\n",
    "cols = ['name.common', 'independent','unMember', 'capital', 'region', 'subregion', 'language', 'latlng', 'landlocked',\n",
    "        'area', 'population', 'gini.2017', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "\n",
    "# 4. Especifica aqu√≠ el nombre del pa√≠s para hacer la predicci√≥n\n",
    "pais = \"Chile\"  # Cambia \"Chile\" por el nombre del pa√≠s que deseas predecir\n",
    "\n",
    "# 5. Cargar el conjunto de datos de prueba (si tienes uno)\n",
    "df_test = pd.read_csv('paises_con_borders.csv')  # Reemplaza con tu archivo de prueba\n",
    "\n",
    "# 6. Verificar que las columnas necesarias est√°n presentes en df_test\n",
    "if not all(col in df_test.columns for col in cols):\n",
    "    print(\"üî¥ Algunas columnas necesarias no est√°n presentes en el conjunto de datos de prueba.\")\n",
    "else:\n",
    "    # 7. Filtrar solo las columnas necesarias del conjunto de datos de prueba\n",
    "    df_test = df_test[cols]\n",
    "\n",
    "    # Crear LabelEncoders si no existen\n",
    "    columns_to_encode = ['region', 'subregion', 'language', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "    if not le_dict:  # Si no tenemos encoders cargados, los creamos\n",
    "        le_dict = {}\n",
    "        for col in columns_to_encode:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('Desconocido')  # Reemplazar valores NaN por 'Desconocido'\n",
    "            df[col] = le.fit_transform(df[col])  # Ajustar y transformar los datos\n",
    "            le_dict[col] = le  # Guardar el encoder para futuras predicciones\n",
    "        # Guardar los encoders\n",
    "        joblib.dump(le_dict, 'encoders.pickle')\n",
    "\n",
    "    # 8. Funci√≥n para predecir si un pa√≠s deber√≠a ser miembro de la ONU\n",
    "    def predecir_pais():\n",
    "        # Filtrar el dataframe para obtener los datos del pa√≠s\n",
    "        input_df = df[df['name.common'] == pais]\n",
    "\n",
    "        if input_df.empty:\n",
    "            print(f\"\\nüî¥ No se encontr√≥ el pa√≠s '{pais}' en los datos.\")\n",
    "            return\n",
    "\n",
    "        # Solo tomar las columnas necesarias\n",
    "        input_df = input_df[cols]\n",
    "\n",
    "        # Convertir tipos seg√∫n necesidad\n",
    "        input_df['area'] = input_df['area'].astype(float)\n",
    "        input_df['population'] = input_df['population'].astype(int)\n",
    "        input_df['gini.2017'] = input_df['gini.2017'].fillna(0.0).astype(float)\n",
    "\n",
    "        # Aplicar los mismos encoders\n",
    "        for col in le_dict:\n",
    "            if col in input_df.columns:\n",
    "                le = le_dict[col]\n",
    "                input_df[col] = input_df[col].fillna('Desconocido').astype(str)\n",
    "                input_df[col] = le.transform(input_df[col])\n",
    "\n",
    "        # Asegurar que tenga las columnas correctas\n",
    "        input_df = input_df[model.feature_names_in_]\n",
    "\n",
    "        # Predecir\n",
    "        pred = model.predict(input_df)[0]\n",
    "        print(\"\\n‚û° El modelo predice que este pa√≠s \" +\n",
    "              (\"üü¢ DEBER√çA\" if pred == 1 else \"üî¥ NO DEBER√çA\") + \" ser miembro de las Naciones Unidas.\")\n",
    "\n",
    "        # Asegurarse de que las columnas categ√≥ricas de df_test est√©n correctamente transformadas\n",
    "        for col in le_dict:\n",
    "            if col in df_test.columns:\n",
    "                le = le_dict[col]\n",
    "                df_test[col] = df_test[col].fillna('Desconocido').astype(str)\n",
    "                df_test[col] = le.transform(df_test[col])\n",
    "\n",
    "        # Convertir las columnas num√©ricas de texto a n√∫meros\n",
    "        df_test['area'] = df_test['area'].astype(float)\n",
    "        df_test['population'] = df_test['population'].astype(int)\n",
    "        df_test['gini.2017'] = df_test['gini.2017'].fillna(0.0).astype(float)\n",
    "\n",
    "        # Asegurarse de que las columnas est√©n alineadas con las caracter√≠sticas de entrada del modelo\n",
    "        df_test = df_test[model.feature_names_in_]\n",
    "\n",
    "        # Verificar que la columna 'unMember' est√© en el conjunto de prueba\n",
    "        if 'unMember' not in df_test.columns:\n",
    "            print(\"üî¥ La columna 'unMember' no est√° presente en el conjunto de prueba.\")\n",
    "            return\n",
    "\n",
    "        # Calcular y mostrar las m√©tricas de clasificaci√≥n en el conjunto de datos de prueba\n",
    "        y_true = df_test['unMember']  # Aseg√∫rate de que la columna 'unMember' est√© en tus datos de prueba\n",
    "        y_pred = model.predict(df_test)\n",
    "\n",
    "        print(\"\\nM√©tricas de rendimiento:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # 9. Ejecutar la funci√≥n\n",
    "    predecir_pais()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b350e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚û° El modelo predice que este pa√≠s üî¥ NO DEBER√çA ser miembro de las Naciones Unidas.\n",
      "\n",
      "üìä M√©tricas del modelo:\n",
      "Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00        58\n",
      "        True       1.00      1.00      1.00       192\n",
      "\n",
      "    accuracy                           1.00       250\n",
      "   macro avg       1.00      1.00      1.00       250\n",
      "weighted avg       1.00      1.00      1.00       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Cargar modelo y encoders\n",
    "def cargar_modelo_y_encoders():\n",
    "    try:\n",
    "        model = joblib.load(\"modelo_un.pickle\")\n",
    "        le_dict = joblib.load(\"encoders.pickle\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"üî¥ No se encontraron los archivos del modelo o los encoders guardados.\")\n",
    "        model = None\n",
    "        le_dict = {}\n",
    "    return model, le_dict\n",
    "\n",
    "# 2. Preparar el dataset\n",
    "def preparar_datos(filepath, cols, le_dict):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    if not all(col in df.columns for col in cols):\n",
    "        print(\"üî¥ Algunas columnas necesarias no est√°n presentes en el conjunto de datos.\")\n",
    "        return None, None\n",
    "\n",
    "    df = df[cols].copy()\n",
    "    \n",
    "    # Codificar variables categ√≥ricas\n",
    "    columns_to_encode = ['region', 'subregion', 'language', 'car.side', 'timezones', 'continents', 'startOfWeek', 'latlng']\n",
    "    if not le_dict:\n",
    "        le_dict = {}\n",
    "        for col in columns_to_encode:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('Desconocido').astype(str)\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            le_dict[col] = le\n",
    "        joblib.dump(le_dict, 'encoders.pickle')\n",
    "    else:\n",
    "        for col in le_dict:\n",
    "            if col in df.columns:\n",
    "                le = le_dict[col]\n",
    "                df[col] = df[col].fillna('Desconocido').astype(str)\n",
    "                df[col] = le.transform(df[col])\n",
    "\n",
    "    # Asegurar tipo num√©rico\n",
    "    df['area'] = df['area'].astype(float)\n",
    "    df['population'] = df['population'].astype(int)\n",
    "\n",
    "    return df, le_dict\n",
    "\n",
    "# 3. Funci√≥n de predicci√≥n\n",
    "def predecir_pais(pais, model, df_full, le_dict):\n",
    "    input_df = df_full[df_full['name.common'] == pais].copy()\n",
    "\n",
    "    if input_df.empty:\n",
    "        print(f\"\\nüî¥ No se encontr√≥ el pa√≠s '{pais}' en los datos.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        input_df = input_df[model.feature_names_in_]\n",
    "        pred = model.predict(input_df)[0]\n",
    "        print(\"\\n‚û° El modelo predice que este pa√≠s \" +\n",
    "              (\"üü¢ DEBER√çA\" if pred == 1 else \"üî¥ NO DEBER√çA\") +\n",
    "              \" ser miembro de las Naciones Unidas.\")\n",
    "    except Exception as e:\n",
    "        print(\"üî¥ Error durante la predicci√≥n:\", e)\n",
    "\n",
    "# 4. Evaluaci√≥n del modelo completo\n",
    "def evaluar_modelo(model, df_test):\n",
    "    if 'unMember' not in df_test.columns:\n",
    "        print(\"üî¥ La columna 'unMember' no est√° presente en el conjunto de prueba.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        y_true = df_test['unMember'].copy()  # ‚Üê Guardar antes de eliminar columnas\n",
    "        df_test = df_test[model.feature_names_in_]\n",
    "        y_pred = model.predict(df_test)\n",
    "\n",
    "        print(\"\\nüìä M√©tricas del modelo:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    except Exception as e:\n",
    "        print(\"üî¥ Error al evaluar el modelo:\", e)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Ejecutar todo el pipeline\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    cols = ['name.common', 'independent', 'unMember', 'capital', 'region', 'subregion', 'language', 'latlng',\n",
    "            'landlocked', 'area', 'population', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "    pais = \"Hong Kong\"  # Cambia el pa√≠s aqu√≠ si lo deseas\n",
    "    filepath = 'paises_con_borders.csv'\n",
    "\n",
    "    model, le_dict = cargar_modelo_y_encoders()\n",
    "\n",
    "    if model:\n",
    "        df_full, le_dict = preparar_datos(filepath, cols, le_dict)\n",
    "        if df_full is not None:\n",
    "            predecir_pais(pais, model, df_full, le_dict)\n",
    "            evaluar_modelo(model, df_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b6140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name.common  independent  unMember     capital  region  subregion  \\\n",
      "0          Botswana         True      True    Gaborone       0         19   \n",
      "1             Tonga         True      True  Nuku'alofa       5         15   \n",
      "2            Greece         True      True      Athens       4         21   \n",
      "3  Marshall Islands         True      True      Majuro       5         10   \n",
      "4           Belarus         True      True       Minsk       4          8   \n",
      "\n",
      "   language  latlng  landlocked      area  population  \\\n",
      "0        75      32        True  582000.0     2351625   \n",
      "1        74      24       False     747.0      105697   \n",
      "2        93     164       False  131990.0    10715549   \n",
      "3        62     248       False     181.0       59194   \n",
      "4        18     216        True  207600.0     9398861   \n",
      "\n",
      "                   borders  car.side  timezones  continents  startOfWeek  \n",
      "0       NAM, ZAF, ZMB, ZWE         0          4           0            0  \n",
      "1                      NaN         0         28           6            0  \n",
      "2       ALB, BGR, TUR, MKD         1          4           3            0  \n",
      "3                      NaN         1         26           6            0  \n",
      "4  LVA, LTU, POL, RUS, UKR         1          5           3            0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Cargar tu conjunto de datos\n",
    "df = pd.read_csv('paises_con_borders.csv')\n",
    "\n",
    "# 2. Crear un diccionario de LabelEncoders para cada columna categ√≥rica\n",
    "columns_to_encode = ['region', 'subregion', 'language', 'car.side', 'timezones', 'continents', 'startOfWeek', 'latlng']\n",
    "le_dict = {}\n",
    "\n",
    "# 3. Ajustar LabelEncoder para cada columna categ√≥rica\n",
    "for col in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = df[col].fillna('Desconocido').astype(str)  # Reemplazar NaN y asegurar tipo string\n",
    "    df[col] = le.fit_transform(df[col])  # Ajustar y transformar los datos\n",
    "    le_dict[col] = le  # Guardar el encoder para futuras predicciones\n",
    "\n",
    "# 4. Guardar los encoders para usarlos en el futuro\n",
    "joblib.dump(le_dict, 'encoders.pickle')\n",
    "\n",
    "# 5. Verifica que el DataFrame ahora tiene las columnas transformadas\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8464e51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚û° El modelo predice que este pa√≠s üî¥ NO DEBER√çA ser miembro de las Naciones Unidas.\n",
      "\n",
      "üìä M√©tricas del modelo:\n",
      "Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00        58\n",
      "        True       1.00      1.00      1.00       192\n",
      "\n",
      "    accuracy                           1.00       250\n",
      "   macro avg       1.00      1.00      1.00       250\n",
      "weighted avg       1.00      1.00      1.00       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Cargar modelo y encoders\n",
    "def cargar_modelo_y_encoders():\n",
    "    try:\n",
    "        model = joblib.load(\"modelo_un.pickle\")\n",
    "        le_dict = joblib.load(\"encoders.pickle\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"üî¥ No se encontraron los archivos del modelo o los encoders guardados.\")\n",
    "        model = None\n",
    "        le_dict = {}\n",
    "    return model, le_dict\n",
    "\n",
    "# 2. Preparar el dataset\n",
    "def preparar_datos(filepath, cols, le_dict):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    if not all(col in df.columns for col in cols):\n",
    "        print(\"üî¥ Algunas columnas necesarias no est√°n presentes en el conjunto de datos.\")\n",
    "        return None, None, None\n",
    "\n",
    "    df = df[cols].copy()\n",
    "\n",
    "    # Guardar target y quitarlo de las features\n",
    "    y_true = df['unMember'].copy()\n",
    "    df = df.drop(columns=['unMember'])\n",
    "\n",
    "    # Codificar variables categ√≥ricas\n",
    "    columns_to_encode = ['region', 'subregion', 'language', 'car.side', 'timezones', 'continents', 'startOfWeek', 'latlng']\n",
    "    if not le_dict:\n",
    "        le_dict = {}\n",
    "        for col in columns_to_encode:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('Desconocido').astype(str)\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            le_dict[col] = le\n",
    "        joblib.dump(le_dict, 'encoders.pickle')\n",
    "    else:\n",
    "        for col in le_dict:\n",
    "            if col in df.columns:\n",
    "                le = le_dict[col]\n",
    "                df[col] = df[col].fillna('Desconocido').astype(str)\n",
    "                df[col] = le.transform(df[col])\n",
    "\n",
    "    # Asegurar tipo num√©rico\n",
    "    df['area'] = df['area'].astype(float)\n",
    "    df['population'] = df['population'].astype(int)\n",
    "\n",
    "    return df, y_true, le_dict\n",
    "\n",
    "# 3. Funci√≥n de predicci√≥n\n",
    "def predecir_pais(pais, model, df_full_original, le_dict):\n",
    "    input_df = df_full_original[df_full_original['name.common'] == pais].copy()\n",
    "\n",
    "    if input_df.empty:\n",
    "        print(f\"\\nüî¥ No se encontr√≥ el pa√≠s '{pais}' en los datos.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        input_df = input_df[model.feature_names_in_]\n",
    "        pred = model.predict(input_df)[0]\n",
    "        print(\"\\n‚û° El modelo predice que este pa√≠s \" +\n",
    "              (\"üü¢ DEBER√çA\" if pred == 1 else \"üî¥ NO DEBER√çA\") +\n",
    "              \" ser miembro de las Naciones Unidas.\")\n",
    "    except Exception as e:\n",
    "        print(\"üî¥ Error durante la predicci√≥n:\", e)\n",
    "\n",
    "# 4. Evaluaci√≥n del modelo completo\n",
    "def evaluar_modelo(model, df_test, y_true):\n",
    "    try:\n",
    "        df_test = df_test[model.feature_names_in_]\n",
    "        y_pred = model.predict(df_test)\n",
    "\n",
    "        print(\"\\nüìä M√©tricas del modelo:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    except Exception as e:\n",
    "        print(\"üî¥ Error al evaluar el modelo:\", e)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Ejecutar todo el pipeline\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    cols = ['name.common', 'independent', 'unMember', 'capital', 'region', 'subregion', 'language', 'latlng',\n",
    "            'landlocked', 'area', 'population', 'car.side', 'timezones', 'continents', 'startOfWeek']\n",
    "    pais = \"Sint Maarten\"  # Cambia el pa√≠s aqu√≠ si lo deseas\n",
    "    filepath = 'paises_con_borders.csv'\n",
    "\n",
    "    model, le_dict = cargar_modelo_y_encoders()\n",
    "\n",
    "    if model:\n",
    "        df_features, y_true, le_dict = preparar_datos(filepath, cols, le_dict)\n",
    "        if df_features is not None:\n",
    "            predecir_pais(pais, model, df_features, le_dict)\n",
    "            evaluar_modelo(model, df_features, y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69122ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo CSV generado: paises_con_lat_lon.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extraer los datos deseados\n",
    "records = []\n",
    "for country in api:\n",
    "    latlng = country.get(\"latlng\", [None, None])\n",
    "    lat = latlng[0] if len(latlng) > 0 else None\n",
    "    lng = latlng[1] if len(latlng) > 1 else None\n",
    "\n",
    "    record = {\n",
    "        \"name.common\": country.get(\"name\", {}).get(\"common\", None),\n",
    "        \"independent\": country.get(\"independent\", None),\n",
    "        \"unMember\": country.get(\"unMember\", None),\n",
    "        \"capital\": \", \".join(country.get(\"capital\", [])) if country.get(\"capital\") else None,\n",
    "        \"region\": country.get(\"region\", None),\n",
    "        \"subregion\": country.get(\"subregion\", None),\n",
    "        \"language\": \", \".join(country.get(\"languages\", {}).values()) if country.get(\"languages\") else None,\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lng,\n",
    "        \"landlocked\": country.get(\"landlocked\", None),\n",
    "        \"area\": country.get(\"area\", None),\n",
    "        \"population\": country.get(\"population\", None),\n",
    "        \"borders\": \", \".join(country.get(\"borders\", [])) if country.get(\"borders\") else None,\n",
    "        \"car.side\": country.get(\"car\", {}).get(\"side\", None),\n",
    "        \"timezones\": \", \".join(country.get(\"timezones\", [])) if country.get(\"timezones\") else None,\n",
    "        \"continents\": \", \".join(country.get(\"continents\", [])) if country.get(\"continents\") else None,\n",
    "        \"startOfWeek\": country.get(\"startOfWeek\", None)\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "# Guardar en archivo CSV\n",
    "with open(\"paises_con_lat_lon.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=records[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(records)\n",
    "\n",
    "print(\"‚úÖ Archivo CSV generado: paises_con_lat_lon.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "837db011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo CSV generado: paises_numericos.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Diccionarios para codificar variables categ√≥ricas\n",
    "region_codes = {}\n",
    "subregion_codes = {}\n",
    "language_codes = {}\n",
    "capital_codes = {}\n",
    "continent_codes = {}\n",
    "start_of_week_codes = {}\n",
    "car_side_codes = {}\n",
    "\n",
    "def get_code(value, code_dict):\n",
    "    if value not in code_dict:\n",
    "        code_dict[value] = len(code_dict) + 1\n",
    "    return code_dict[value]\n",
    "\n",
    "# Extraer y codificar datos\n",
    "records = []\n",
    "for country in api:\n",
    "    latlng = country.get(\"latlng\", [None, None])\n",
    "    lat = latlng[0] if len(latlng) > 0 else None\n",
    "    lng = latlng[1] if len(latlng) > 1 else None\n",
    "\n",
    "    record = {\n",
    "        \"independent\": int(country.get(\"independent\", False)),\n",
    "        \"unMember\": int(country.get(\"unMember\", False)),\n",
    "        \"capital_code\": get_code(\", \".join(country.get(\"capital\", [])) if country.get(\"capital\") else \"None\", capital_codes),\n",
    "        \"region_code\": get_code(country.get(\"region\", \"None\"), region_codes),\n",
    "        \"subregion_code\": get_code(country.get(\"subregion\", \"None\"), subregion_codes),\n",
    "        \"language_code\": get_code(\", \".join(country.get(\"languages\", {}).values()) if country.get(\"languages\") else \"None\", language_codes),\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lng,\n",
    "        \"landlocked\": int(country.get(\"landlocked\", False)),\n",
    "        \"area\": country.get(\"area\", 0),\n",
    "        \"population\": country.get(\"population\", 0),\n",
    "        \"borders_count\": len(country.get(\"borders\", [])),\n",
    "        \"car_side_code\": get_code(country.get(\"car\", {}).get(\"side\", \"None\"), car_side_codes),\n",
    "        \"timezones_count\": len(country.get(\"timezones\", [])),\n",
    "        \"continents_code\": get_code(\", \".join(country.get(\"continents\", [])) if country.get(\"continents\") else \"None\", continent_codes),\n",
    "        \"startOfWeek_code\": get_code(country.get(\"startOfWeek\", \"None\"), start_of_week_codes)\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "# Guardar en archivo CSV\n",
    "with open(\"paises_numericos.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=records[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(records)\n",
    "\n",
    "print(\"‚úÖ Archivo CSV generado: paises_numericos.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Taller_2_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
